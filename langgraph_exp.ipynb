{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd23c5c",
   "metadata": {},
   "source": [
    "# LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e6aeeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini LLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "os.environ.get(\"GOOGLE_API_KEY\")\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e289692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools for agent\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "web_search = DuckDuckGoSearchResults(\n",
    "    max_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6a02ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [web_search]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed16bb6",
   "metadata": {},
   "source": [
    "# Agent Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c558e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langgraph.types import Command \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d981ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State init\n",
    "\n",
    "class ChildState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # decision: Literal[\"research\", \"summarize\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e776f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def agent(state: ChildState):\n",
    "    return {\n",
    "        \"messages\": [llm_with_tools.invoke(state[\"messages\"])], \n",
    "    }\n",
    "\n",
    "def tools_router(state: ChildState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if(hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0):\n",
    "        return \"tool_node\"\n",
    "    else: \n",
    "        return END\n",
    "    \n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "subgraph = StateGraph(ChildState)\n",
    "\n",
    "subgraph.add_node(\"agent\", agent)\n",
    "subgraph.add_node(\"tool_node\", tool_node)\n",
    "subgraph.set_entry_point(\"agent\")\n",
    "\n",
    "subgraph.add_conditional_edges(\"agent\", tools_router)\n",
    "subgraph.add_edge(\"tool_node\", \"agent\")\n",
    "\n",
    "search_app = subgraph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb90deee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        search_app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27601006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Chennai is partly cloudy with a maximum temperature of 37 degrees Celsius and a minimum of 29 degrees Celsius. Rain is expected from May 5 (Monday).\n"
     ]
    }
   ],
   "source": [
    "response=search_app.invoke({\"messages\": [HumanMessage(content=\"How is the weather in Chennai?\")]})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00c1cb",
   "metadata": {},
   "source": [
    "# Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326538c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, List, Literal \n",
    "from pydantic import BaseModel, Field \n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.types import Command \n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.prebuilt import create_react_agent \n",
    "from IPython.display import Image, display \n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# tools for agent\n",
    "\n",
    "\n",
    "os.environ.get(\"GOOGLE_API_KEY\")\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "\n",
    "web_search = DuckDuckGoSearchResults(\n",
    "    max_results=3,\n",
    ")\n",
    "\n",
    "\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3ee8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSummarizer(BaseModel):\n",
    "    next: Literal[\"supervisor\"] = Field(\n",
    "        description=\"Based on the user's input, summarize the task\"\n",
    "                    \"Provide an action plan using Chain of Thought reasoning\"\n",
    "                    \"and provide a list of steps to complete the task.\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"Detailed justification for the routing decision, explaining the rationale behind selecting the particular specialist and how this advances the task toward completion.\"\n",
    "    )    \n",
    "\n",
    "\n",
    "def task_summarizer(state: MessagesState)-> Command[Literal[\"supervisor\"]]:\n",
    "    system_prompt = \"\"\"\n",
    "    You are a task summarizer.\n",
    "    Based on the user's input, summarize the task.\n",
    "    Provide an action plan using Chain of Thought reasoning\n",
    "    and provide a list of steps to complete the task.\n",
    "    Return only one of the following values for `next`:\n",
    "        - 'supervisor'\n",
    "\n",
    "        Respond using structured output only — do not explain anything outside of the `reason` field.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  \n",
    "    ] + state[\"messages\"] \n",
    "\n",
    "    response = llm.with_structured_output(TaskSummarizer).invoke(messages)\n",
    "\n",
    "    goto = response.next\n",
    "    reason = response.reason\n",
    "\n",
    "    # print(f\"--- Workflow Transition: tasl_summarizer → {goto.upper()} ---\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=reason, name=\"task_summarizer\"),\n",
    "            ]\n",
    "        },\n",
    "        goto=goto,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f51dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor(BaseModel):\n",
    "    next: Literal[\"researcher\", \"coder\"] = Field(\n",
    "        description=\"Determines which specialist to activate next in the workflow sequence: \"\n",
    "                    \"'researcher' when additional facts, context, or data collection is necessary, \"\n",
    "                    \"'coder' when implementation, computation, or technical problem-solving is required.\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"Detailed justification for the routing decision, explaining the rationale behind selecting the particular specialist and how this advances the task toward completion.\"\n",
    "    )\n",
    "\n",
    "def supervisor_node(state: MessagesState) -> Command[Literal[\"researcher\", \"coder\"]]:\n",
    "\n",
    "    system_prompt = ('''\n",
    "                 \n",
    "        You are a workflow supervisor managing a team of three specialized agents: Researcher, and Coder. Your role is to orchestrate the workflow by selecting the most appropriate next agent based on the current state and needs of the task. Provide a clear, concise rationale for each decision to ensure transparency in your decision-making process.\n",
    "\n",
    "        **Team Members**:\n",
    "        1. **Researcher**: Specializes in information gathering, fact-finding, and collecting relevant data needed to address the user's request.\n",
    "        2. **Coder**: Focuses on technical implementation, calculations, data analysis, algorithm development, and coding solutions.\n",
    "\n",
    "        **Your Responsibilities**:\n",
    "        1. Analyze each user request and agent response for completeness, accuracy, and relevance.\n",
    "        2. Route the task to the most appropriate agent at each decision point.\n",
    "        3. Maintain workflow momentum by avoiding redundant agent assignments.\n",
    "        4. Continue the process until the user's request is fully and satisfactorily resolved.\n",
    "\n",
    "        Your objective is to create an efficient workflow that leverages each agent's strengths while minimizing unnecessary steps, ultimately delivering complete and accurate solutions to user requests.\n",
    "                 \n",
    "    ''')\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},  \n",
    "    ] + state[\"messages\"] \n",
    "\n",
    "    response = llm.with_structured_output(Supervisor).invoke(messages)\n",
    "\n",
    "    goto = response.next\n",
    "    reason = response.reason\n",
    "\n",
    "    # print(f\"--- Workflow Transition: Supervisor → {goto.upper()} ---\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=reason, name=\"supervisor\")\n",
    "            ]\n",
    "        },\n",
    "        goto=goto,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee3369e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enhancer_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "\n",
    "#     \"\"\"\n",
    "#         Enhancer agent node that improves and clarifies user queries.\n",
    "#         Takes the original user input and transforms it into a more precise,\n",
    "#         actionable request before passing it to the supervisor.\n",
    "#     \"\"\"\n",
    "   \n",
    "#     system_prompt = (\n",
    "#         \"You are a Query Refinement Specialist with expertise in transforming vague requests into precise instructions. Your responsibilities include:\\n\\n\"\n",
    "#         \"1. Analyzing the original query to identify key intent and requirements\\n\"\n",
    "#         \"2. Resolving any ambiguities without requesting additional user input\\n\"\n",
    "#         \"3. Expanding underdeveloped aspects of the query with reasonable assumptions\\n\"\n",
    "#         \"4. Restructuring the query for clarity and actionability\\n\"\n",
    "#         \"5. Ensuring all technical terminology is properly defined in context\\n\\n\"\n",
    "#         \"Important: Never ask questions back to the user. Instead, make informed assumptions and create the most comprehensive version of their request possible.\"\n",
    "#     )\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},  \n",
    "#     ] + state[\"messages\"]  \n",
    "\n",
    "#     enhanced_query = llm.invoke(messages)\n",
    "\n",
    "#     print(f\"--- Workflow Transition: Prompt Enhancer → Supervisor ---\")\n",
    "\n",
    "#     return Command(\n",
    "#         update={\n",
    "#             \"messages\": [  \n",
    "#                 HumanMessage(\n",
    "#                     content=enhanced_query.content, \n",
    "#                     name=\"enhancer\"  \n",
    "#                 )\n",
    "#             ]\n",
    "#         },\n",
    "#         goto=\"supervisor\", \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6c9871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state: MessagesState):\n",
    "\n",
    "    \"\"\"\n",
    "        Research agent node that gathers information using Tavily search.\n",
    "        Takes the current task state, performs relevant research,\n",
    "        and returns findings for validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    research_agent = create_react_agent(\n",
    "        llm,  \n",
    "        tools=[web_search],  \n",
    "        state_modifier= \"You are an Information Specialist with expertise in comprehensive research. Your responsibilities include:\\n\\n\"\n",
    "            \"1. Identifying key information needs based on the query context\\n\"\n",
    "            \"2. You must use search web tool to gathering relevant, accurate, and up-to-date information from reliable sources\\n\"\n",
    "            \"3. You must give the final result to answer user question by organizing findings in a structured, easily digestible format\\n\"\n",
    "            \"4. Citing sources when possible to establish credibility\\n\"\n",
    "            \"5. Focusing exclusively on information gathering - avoid analysis or implementation\\n\\n\"\n",
    "            \"Provide thorough, factual responses without speculation where information is unavailable.\"\n",
    "    )\n",
    "\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    # print(f\"--- Workflow Transition: Researcher → end ---\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [ \n",
    "                HumanMessage(\n",
    "                    content=result[\"messages\"][-1].content,  \n",
    "                    name=\"researcher\"  \n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        goto=END, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad4af7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_node(state: MessagesState):\n",
    "\n",
    "    code_agent = create_react_agent(\n",
    "        llm,\n",
    "        tools=[python_repl_tool],\n",
    "        state_modifier=(\n",
    "            \"You are a coder and analyst. Focus on mathematical calculations, analyzing, solving math questions, \"\n",
    "            \"and executing code. Handle technical problem-solving and data tasks.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    result = code_agent.invoke(state)\n",
    "\n",
    "    # print(f\"--- Workflow Transition: Coder → end ---\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n",
    "            ]\n",
    "        },\n",
    "        goto=END,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c326f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # System prompt providing clear instructions to the validator agent\n",
    "# system_prompt = '''\n",
    "#     Your task is to ensure reasonable quality. \n",
    "#     Specifically, you must:\n",
    "#     - Review the user's question (the first message in the workflow).\n",
    "#     - Review the answer (the last message in the workflow).\n",
    "#     - If the answer addresses the core intent of the question, even if not perfectly, signal to end the workflow with 'FINISH'.\n",
    "#     - Only route back to the supervisor if the answer is completely off-topic, harmful, or fundamentally misunderstands the question.\n",
    "    \n",
    "#     - Accept answers that are \"good enough\" rather than perfect\n",
    "#     - Prioritize workflow completion over perfect responses\n",
    "#     - Give benefit of doubt to borderline answers\n",
    "    \n",
    "#     Routing Guidelines:\n",
    "#     1. 'supervisor' Agent: ONLY for responses that are completely incorrect or off-topic.\n",
    "#     2. Respond with 'FINISH' in all other cases to end the workflow.\n",
    "# '''\n",
    "\n",
    "# class Validator(BaseModel):\n",
    "#     next: Literal[\"supervisor\", \"FINISH\"] = Field(\n",
    "#         description=\"Specifies the next worker in the pipeline: 'supervisor' to continue or 'FINISH' to terminate.\"\n",
    "#     )\n",
    "#     reason: str = Field(\n",
    "#         description=\"The reason for the decision.\"\n",
    "#     )\n",
    "\n",
    "# def validator_node(state: MessagesState) -> Command[Literal[\"supervisor\", \"__end__\"]]:\n",
    "\n",
    "#     user_question = state[\"messages\"][0].content\n",
    "#     agent_answer = state[\"messages\"][-1].content\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": user_question},\n",
    "#         {\"role\": \"assistant\", \"content\": agent_answer},\n",
    "#     ]\n",
    "\n",
    "#     response = llm.with_structured_output(Validator).invoke(messages)\n",
    "\n",
    "#     goto = response.next\n",
    "#     reason = response.reason\n",
    "\n",
    "#     if goto == \"FINISH\" or goto == END:\n",
    "#         goto = END  \n",
    "#         print(\" --- Transitioning to END ---\")  \n",
    "#     else:\n",
    "#         print(f\"--- Workflow Transition: Validator → Supervisor ---\")\n",
    " \n",
    "\n",
    "#     return Command(\n",
    "#         update={\n",
    "#             \"messages\": [\n",
    "#                 HumanMessage(content=reason, name=\"validator\")\n",
    "#             ]\n",
    "#         },\n",
    "#         goto=goto, \n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "838d5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "graph.add_node(\"task_summarizer\", task_summarizer)\n",
    "graph.add_node(\"supervisor\", supervisor_node) \n",
    "# graph.add_node(\"enhancer\", enhancer_node)  \n",
    "graph.add_node(\"researcher\", research_node) \n",
    "graph.add_node(\"coder\", code_node) \n",
    "# graph.add_node(\"validator\", validator_node)  \n",
    "\n",
    "graph.add_edge(START, \"task_summarizer\")  \n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b071516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Image\n",
    "\n",
    "# display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61eb21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_message(message, indent=False):\n",
    "    pretty_message = message.pretty_repr(html=True)\n",
    "    if not indent:\n",
    "        print(pretty_message)\n",
    "        return\n",
    "\n",
    "    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n",
    "    print(indented)\n",
    "\n",
    "\n",
    "def pretty_print_messages(update, last_message=False):\n",
    "    is_subgraph = False\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "        is_subgraph = True\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        update_label = f\"Update from node {node_name}:\"\n",
    "        if is_subgraph:\n",
    "            update_label = \"\\t\" + update_label\n",
    "\n",
    "        print(update_label)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        messages = convert_to_messages(node_update[\"messages\"])\n",
    "        if last_message:\n",
    "            messages = messages[-1:]\n",
    "\n",
    "        for m in messages:\n",
    "            pretty_print_message(m, indent=is_subgraph)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c66d63b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node task_summarizer:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: task_summarizer\n",
      "\n",
      "The user is asking for the weather in Chennai. This requires me to route the request to a specialist, specifically a weather expert, to fetch the weather information for the specified location. The next step is to involve the 'supervisor' to find the weather expert..\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: supervisor\n",
      "\n",
      "The user is asking a question that requires gathering information. The researcher is best suited to find the weather information for Chennai. \n",
      "\n",
      "\n",
      "Update from node researcher:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: researcher\n",
      "\n",
      "The weather forecast for Chennai indicates partly cloudy skies with a maximum temperature of 37 degrees Celsius and a minimum of 29 degrees Celsius. Rainfall is expected from May 5 (Monday).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Weather in Chennai\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "for event in app.stream(inputs):\n",
    "    pretty_print_messages(event)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d84878f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20th Fibonacci number is 6765.\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": [\"Give me the 20th fibonacci number\"]\n",
    "}\n",
    "\n",
    "\n",
    "events = app.astream_events(input=input, version=\"v2\")\n",
    "\n",
    "async for event in events: \n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74c099f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the search results, here's what I found regarding Donald Trump in 2025:\n",
      "\n",
      "*   **Second Presidency:** Some sources refer to a timeline of Donald Trump's second presidency, starting with his inauguration on January 20, 2025.\n",
      "*   **First 100 Days:** Reports mention Trump securing trillions in new U.S.-based investments and job creation within his first 100 days.\n",
      "*   **Project 2025:** This is a policy plan from the Heritage Foundation that aligns with many of Trump's policies during his second term, focusing on expanding presidential power and conservative social visions.\n",
      "*   **News Events:** News articles from CNN and other sources detail specific events during his presidency in 2025, such as executive orders and reactions to his dialogues with other world leaders.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": [\"Hi, research about Donal Trump in 2025\"]\n",
    "}\n",
    "\n",
    "events = app.astream_events(input=input, version=\"v2\")\n",
    "\n",
    "async for event in events: \n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c680d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
